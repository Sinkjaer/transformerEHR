{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Move files to datastore and make smaller folder that has only the necessary files\n",
        "\n",
        "from azureml.core import Workspace, Experiment, Environment\n",
        "from azureml.core.script_run_config import ScriptRunConfig\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# Retrieve Workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Get the Curated Environment\n",
        "env = Environment.get(workspace=ws, name=\"AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\")\n",
        "\n",
        "# Clone the environment to modify it\n",
        "env_clone = env.clone(new_name=\"my-custom-environment\")\n",
        "\n",
        "# Define additional conda and pip packages you want to install\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package('torchtext') # Add the torchtext package\n",
        "\n",
        "# Add the defined dependencies to the cloned environment\n",
        "env_clone.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Define the Training Script and Directory\n",
        "script_folder = ''\n",
        "script_name = 'task/executeBehrtMLM.py'  # your training script\n",
        "\n",
        "# Create an Experiment\n",
        "experiment_name = 'My-Experiment'\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\n",
        "\n",
        "# Configure and Submit the Training Job\n",
        "src = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script=script_name,\n",
        "                      environment=env_clone)  # specifying the cloned environment here\n",
        "\n",
        "run = experiment.submit(src)\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "SnapshotException",
          "evalue": "SnapshotException:\n\tMessage: ====================================================================\n\nWhile attempting to take snapshot of /mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR\nYour total snapshot size exceeds the limit of 300.0 MB.\nPlease see http://aka.ms/aml-largefiles on how to work with large files.\n\n====================================================================\n\n\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"====================================================================\\n\\nWhile attempting to take snapshot of /mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR\\nYour total snapshot size exceeds the limit of 300.0 MB.\\nPlease see http://aka.ms/aml-largefiles on how to work with large files.\\n\\n====================================================================\\n\\n\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSnapshotException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Configure and Submit the Training Job\u001b[39;00m\n\u001b[1;32m     30\u001b[0m src \u001b[38;5;241m=\u001b[39m ScriptRunConfig(source_directory\u001b[38;5;241m=\u001b[39mscript_folder,\n\u001b[1;32m     31\u001b[0m                       script\u001b[38;5;241m=\u001b[39mscript_name,\n\u001b[1;32m     32\u001b[0m                       environment\u001b[38;5;241m=\u001b[39menv_clone)  \u001b[38;5;66;03m# specifying the cloned environment here\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m run\u001b[38;5;241m.\u001b[39mwait_for_completion(show_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/core/experiment.py:238\u001b[0m, in \u001b[0;36mExperiment.submit\u001b[0;34m(self, config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m submit_func \u001b[38;5;241m=\u001b[39m get_experiment_submit(config)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmit config \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)):\n\u001b[0;32m--> 238\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43msubmit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     run\u001b[38;5;241m.\u001b[39mset_tags(tags)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/core/script_run_config.py:63\u001b[0m, in \u001b[0;36msubmit\u001b[0;34m(script_run_config, workspace, experiment_name, run_id, _parent_run_id, credential_passthrough)\u001b[0m\n\u001b[1;32m     60\u001b[0m inputs, _ \u001b[38;5;241m=\u001b[39m _update_args_and_io(workspace, run_config)\n\u001b[1;32m     61\u001b[0m collect_datasets_usage(module_logger, _SCRIPT_RUN_SUBMIT_ACTIVITY, inputs,\n\u001b[1;32m     62\u001b[0m                        workspace, run_config\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m---> 63\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43m_commands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtelemetry_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_run_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_run_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parent_run_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m run\u001b[38;5;241m.\u001b[39madd_properties(global_tracking_info_registry\u001b[38;5;241m.\u001b[39mgather_all(script_run_config\u001b[38;5;241m.\u001b[39msource_directory))\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/_execution/_commands.py:117\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(project_object, run_config_object, run_id, injected_files, telemetry_values, parent_run_id, prepare_only, check)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prepare_only \u001b[38;5;129;01mand\u001b[39;00m check:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExperimentExecutionException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not check preparation of local targets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_internal_local_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshared_start_run_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _start_internal(project_object, run_config_object, prepare_check\u001b[38;5;241m=\u001b[39mcheck,\n\u001b[1;32m    121\u001b[0m                            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_start_run_kwargs)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/_execution/_commands.py:282\u001b[0m, in \u001b[0;36m_start_internal_local_cloud\u001b[0;34m(project_object, run_config_object, prepare_only, custom_target_dict, run_id, injected_files, telemetry_values, parent_run_id)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExperimentExecutionException(ex\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m--> 282\u001b[0m snapshot_id \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_async\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m snapshot_async \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    283\u001b[0m thread_pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_run_details(project_object, run_config_object, run_id,\n\u001b[1;32m    286\u001b[0m                         snapshot_id\u001b[38;5;241m=\u001b[39msnapshot_id)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/_restclient/snapshots_client.py:87\u001b[0m, in \u001b[0;36mSnapshotsClient.create_snapshot\u001b[0;34m(self, file_or_folder_path, retry_on_failure, raise_on_validation_failure)\u001b[0m\n\u001b[1;32m     84\u001b[0m ignore_file \u001b[38;5;241m=\u001b[39m get_project_ignore_file(file_or_folder_path)\n\u001b[1;32m     85\u001b[0m exclude_function \u001b[38;5;241m=\u001b[39m ignore_file\u001b[38;5;241m.\u001b[39mis_file_excluded\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_snapshot_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_or_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_validation_failure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Get the previous snapshot for this project\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/_restclient/snapshots_client.py:65\u001b[0m, in \u001b[0;36mSnapshotsClient._validate_snapshot_size\u001b[0;34m(self, file_or_folder_path, exclude_function, raise_on_validation_failure)\u001b[0m\n\u001b[1;32m     56\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m====================================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhile attempting to take snapshot of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m====================================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file_or_folder_path, SNAPSHOT_MAX_SIZE_BYTES \u001b[38;5;241m/\u001b[39m ONE_MB)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_on_validation_failure:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnapshotException(error_message)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarning(error_message)\n",
            "\u001b[0;31mSnapshotException\u001b[0m: SnapshotException:\n\tMessage: ====================================================================\n\nWhile attempting to take snapshot of /mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR\nYour total snapshot size exceeds the limit of 300.0 MB.\nPlease see http://aka.ms/aml-largefiles on how to work with large files.\n\n====================================================================\n\n\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"====================================================================\\n\\nWhile attempting to take snapshot of /mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR\\nYour total snapshot size exceeds the limit of 300.0 MB.\\nPlease see http://aka.ms/aml-largefiles on how to work with large files.\\n\\n====================================================================\\n\\n\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691765607925
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "# sys.path.insert(0, \"../\")\n",
        "from common.common import create_folder\n",
        "from dataLoader.build_vocab import build_vocab\n",
        "import pytorch_pretrained_bert as Bert\n",
        "from dataLoader.dataLoaderMLM import MaskedDataset\n",
        "from model.behrt import BertModel, BertMLM\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import os\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.loggers import NeptuneLogger\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.tuner import Tuner\n",
        "\n",
        "Azure = True\n",
        "\n",
        "# Initialize Neptune\n",
        "name_experiment = \"MLM_model\"\n",
        "neptune_logger = NeptuneLogger(\n",
        "    project=\"sinkjaer/BEHRT\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzOWVmOWI3Mi1jNjliLTQ3NmEtODVjMy0wZjkxZTBiMzFiMzEifQ==\",\n",
        "    log_model_checkpoints=True,\n",
        "    name=name_experiment,\n",
        ")\n",
        "\n",
        "\n",
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get(\"vocab_size\"),\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            num_hidden_layers=config.get(\"num_hidden_layers\"),\n",
        "            num_attention_heads=config.get(\"num_attention_heads\"),\n",
        "            intermediate_size=config.get(\"intermediate_size\"),\n",
        "            hidden_act=config.get(\"hidden_act\"),\n",
        "            hidden_dropout_prob=config.get(\"hidden_dropout_prob\"),\n",
        "            attention_probs_dropout_prob=config.get(\"attention_probs_dropout_prob\"),\n",
        "            max_position_embeddings=config.get(\"max_position_embedding\"),\n",
        "            initializer_range=config.get(\"initializer_range\"),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get(\"seg_vocab_size\")\n",
        "        self.age_vocab_size = config.get(\"age_vocab_size\")\n",
        "        self.date_vocab_size = config.get(\"date_vocab_size\")\n",
        "        self.optim_param = config.get(\"optim_param\")\n",
        "\n",
        "\n",
        "if Azure:\n",
        "    os.environ['NEPTUNE_MODE'] = 'offline'\n",
        "    file_config = {\n",
        "        \"data_train\": \"../../EHR_data/data/pre_train_training_set.json\",  # formated data\n",
        "        \"data_val\": \"../../EHR_data/data/pre_train_validation_set.json\",  # formated data\n",
        "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
        "        \"model_name\": \"behrt\",  # model name\n",
        "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "    }\n",
        "else:\n",
        "    file_config = {\n",
        "        \"data_train\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
        "        \"data_val\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
        "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
        "        \"model_name\": \"behrt\",  # model name\n",
        "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "    }\n",
        "\n",
        "create_folder(file_config[\"model_path\"])\n",
        "\n",
        "global_params = {\"max_seq_len\": 512, \"gradient_accumulation_steps\": 1}\n",
        "\n",
        "optim_param = {\"lr\": 2e-5, \"warmup_proportion\": 0.1, \"weight_decay\": 0.01}\n",
        "\n",
        "train_params = {\n",
        "    \"batch_size\": 128,\n",
        "    \"max_len_seq\": global_params[\"max_seq_len\"],\n",
        "}\n",
        "\n",
        "# load data\n",
        "with open(file_config[\"data_train\"]) as f:\n",
        "    data_train_json = json.load(f)\n",
        "with open(file_config[\"data_val\"]) as f:\n",
        "    data_val_json = json.load(f)\n",
        "\n",
        "# Build vocab\n",
        "vocab_path = os.path.join(file_config[\"model_path\"], file_config[\"vocab\"])\n",
        "vocab_list, word_to_idx = build_vocab(\n",
        "    data_train_json,\n",
        "    save_file=vocab_path,\n",
        ")\n",
        "\n",
        "# Data loader\n",
        "masked_data_train = MaskedDataset(data_train_json, vocab_list, word_to_idx)\n",
        "trainload = DataLoader(\n",
        "    dataset=masked_data_train,\n",
        "    batch_size=train_params[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=6,\n",
        ")\n",
        "masked_data_val = MaskedDataset(data_val_json, vocab_list, word_to_idx)\n",
        "valload = DataLoader(\n",
        "    dataset=masked_data_val,\n",
        "    batch_size=train_params[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=6,\n",
        ")\n",
        "\n",
        "# Model config\n",
        "model_config = {\n",
        "    \"vocab_size\": len(vocab_list),  # number of disease + symbols for word embedding\n",
        "    \"hidden_size\": 288,  # word embedding and seg embedding hidden size\n",
        "    \"seg_vocab_size\": 2,  # number of vocab for seg embedding\n",
        "    \"date_vocab_size\": int(\n",
        "        365.25 * 23\n",
        "    ),  # number of vocab for dates embedding --> days in 23 years\n",
        "    \"age_vocab_size\": 144,  # number of vocab for age embedding\n",
        "    \"max_position_embedding\": train_params[\"max_len_seq\"],  # maximum number of tokens\n",
        "    \"hidden_dropout_prob\": 0.1,  # dropout rate\n",
        "    \"num_hidden_layers\": 6,  # number of multi-head attention layers required\n",
        "    \"num_attention_heads\": 12,  # number of attention heads\n",
        "    \"attention_probs_dropout_prob\": 0.1,  # multi-head attention dropout rate\n",
        "    \"intermediate_size\": 512,  # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    \"hidden_act\": \"gelu\",  # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    \"initializer_range\": 0.02,  # parameter weight initializer range\n",
        "    \"optim_param\": optim_param,  # learning rate\n",
        "}\n",
        "\n",
        "# Checkopoint\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"metrics/epoch/loss_val\",\n",
        "    dirpath=file_config[\"model_path\"] + \"/checkpoints\",\n",
        "    filename=\"checkpoint-{epoch:02d}\",\n",
        ")\n",
        "\n",
        "# Define model\n",
        "neptune_logger.log_hyperparams(model_config)\n",
        "model = BertModel(BertConfig(model_config))\n",
        "task = BertMLM(model, BertConfig(model_config))\n",
        "\n",
        "# Initialize the Trainer with the callback and Neptune logger\n",
        "trainer = pl.Trainer(\n",
        "    accelerator = 'gpu',\n",
        "    logger=neptune_logger,\n",
        "    max_epochs=10,\n",
        "    log_every_n_steps=100,\n",
        "    callbacks=checkpoint_callback,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model as usual\n",
        "trainer.fit(model=task, train_dataloaders=trainload, val_dataloaders=valload)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_folder\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_vocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_bert\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mBert\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataLoaderMLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedDataset\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR/dataLoader/build_vocab.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Script to build vocabulary from a dataset and save it.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691762930308
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}