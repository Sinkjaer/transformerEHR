{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "conda = CondaDependencies()\n",
        "\n",
        "# add channels\n",
        "conda.add_channel('pytorch')\n",
        "\n",
        "# add conda packages\n",
        "conda.add_conda_package('python=3.8')\n",
        "conda.add_conda_package('pytorch')\n",
        "conda.add_conda_package('torchvision')\n",
        "\n",
        "# add pip packages\n",
        "conda.add_pip_package('pyyaml')\n",
        "conda.add_pip_package('mpi4py')\n",
        "conda.add_pip_package('deepspeed')\n",
        "\n",
        "# create environment\n",
        "env = Environment('pytorch')\n",
        "env.python.conda_dependencies = conda"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1691751117331
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Create a Python environment for the experiment\n",
        "env = Environment('pytorch-cuda')\n",
        "conda_dep = CondaDependencies()\n",
        "\n",
        "# Define the packages needed by the model and scripts\n",
        "conda_dep.add_conda_package('python=3.8')\n",
        "conda_dep.add_pip_package('pyyaml')\n",
        "conda_dep.add_pip_package('mpi4py')\n",
        "conda_dep.add_pip_package('deepspeed')\n",
        "\n",
        "# Add the dependencies to the environment\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Specify a Docker image that includes CUDA and PyTorch\n",
        "env.docker.enabled = True\n",
        "env.docker.base_image = \"AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\"\n",
        "\n",
        "# Create environment\n",
        "env.register(workspace=ws)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "from azureml.core import Environment\n",
        "\n",
        "env = Environment(\"behrt_gpu\")\n",
        "env.docker.base_image =\"behrt_gpu\"\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691751203005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azureml.core import Environment\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "curated_env_name = 'AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu'\n",
        "pytorch_env = Environment.get(workspace=ws, name=curated_env_name)\n",
        "print(pytorch_env.docker.base_image)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "None\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691751532129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "env = Environment(name='my-env')\n",
        "\n",
        "env.docker.enabled = True\n",
        "env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691751310362
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"../\")\n",
        "from common.common import create_folder\n",
        "from dataLoader.build_vocab import build_vocab\n",
        "import pytorch_pretrained_bert as Bert\n",
        "from dataLoader.dataLoaderMLM import MaskedDataset\n",
        "from model.behrt import BertModel, BertMLM\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import os\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.loggers import NeptuneLogger\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.tuner import Tuner\n",
        "\n",
        "Azure = True\n",
        "\n",
        "# Initialize Neptune\n",
        "name_experiment = \"MLM_model\"\n",
        "neptune_logger = NeptuneLogger(\n",
        "    project=\"sinkjaer/BEHRT\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzOWVmOWI3Mi1jNjliLTQ3NmEtODVjMy0wZjkxZTBiMzFiMzEifQ==\",\n",
        "    log_model_checkpoints=True,\n",
        "    name=name_experiment,\n",
        ")\n",
        "\n",
        "\n",
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get(\"vocab_size\"),\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            num_hidden_layers=config.get(\"num_hidden_layers\"),\n",
        "            num_attention_heads=config.get(\"num_attention_heads\"),\n",
        "            intermediate_size=config.get(\"intermediate_size\"),\n",
        "            hidden_act=config.get(\"hidden_act\"),\n",
        "            hidden_dropout_prob=config.get(\"hidden_dropout_prob\"),\n",
        "            attention_probs_dropout_prob=config.get(\"attention_probs_dropout_prob\"),\n",
        "            max_position_embeddings=config.get(\"max_position_embedding\"),\n",
        "            initializer_range=config.get(\"initializer_range\"),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get(\"seg_vocab_size\")\n",
        "        self.age_vocab_size = config.get(\"age_vocab_size\")\n",
        "        self.date_vocab_size = config.get(\"date_vocab_size\")\n",
        "        self.optim_param = config.get(\"optim_param\")\n",
        "\n",
        "\n",
        "if Azure:\n",
        "    os.environ['NEPTUNE_MODE'] = 'offline'\n",
        "    file_config = {\n",
        "        \"data_train\": \"../../EHR_data/data/pre_train_training_set.json\",  # formated data\n",
        "        \"data_val\": \"../../EHR_data/data/pre_train_validation_set.json\",  # formated data\n",
        "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
        "        \"model_name\": \"behrt\",  # model name\n",
        "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "    }\n",
        "else:\n",
        "    file_config = {\n",
        "        \"data_train\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
        "        \"data_val\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
        "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
        "        \"model_name\": \"behrt\",  # model name\n",
        "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "    }\n",
        "\n",
        "create_folder(file_config[\"model_path\"])\n",
        "\n",
        "global_params = {\"max_seq_len\": 512, \"gradient_accumulation_steps\": 1}\n",
        "\n",
        "optim_param = {\"lr\": 2e-5, \"warmup_proportion\": 0.1, \"weight_decay\": 0.01}\n",
        "\n",
        "train_params = {\n",
        "    \"batch_size\": 128,\n",
        "    \"max_len_seq\": global_params[\"max_seq_len\"],\n",
        "}\n",
        "\n",
        "# load data\n",
        "with open(file_config[\"data_train\"]) as f:\n",
        "    data_train_json = json.load(f)\n",
        "with open(file_config[\"data_val\"]) as f:\n",
        "    data_val_json = json.load(f)\n",
        "\n",
        "# Build vocab\n",
        "vocab_path = os.path.join(file_config[\"model_path\"], file_config[\"vocab\"])\n",
        "vocab_list, word_to_idx = build_vocab(\n",
        "    data_train_json,\n",
        "    save_file=vocab_path,\n",
        ")\n",
        "\n",
        "# Data loader\n",
        "masked_data_train = MaskedDataset(data_train_json, vocab_list, word_to_idx)\n",
        "trainload = DataLoader(\n",
        "    dataset=masked_data_train,\n",
        "    batch_size=train_params[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=6,\n",
        ")\n",
        "masked_data_val = MaskedDataset(data_val_json, vocab_list, word_to_idx)\n",
        "valload = DataLoader(\n",
        "    dataset=masked_data_val,\n",
        "    batch_size=train_params[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=6,\n",
        ")\n",
        "\n",
        "# Model config\n",
        "model_config = {\n",
        "    \"vocab_size\": len(vocab_list),  # number of disease + symbols for word embedding\n",
        "    \"hidden_size\": 288,  # word embedding and seg embedding hidden size\n",
        "    \"seg_vocab_size\": 2,  # number of vocab for seg embedding\n",
        "    \"date_vocab_size\": int(\n",
        "        365.25 * 23\n",
        "    ),  # number of vocab for dates embedding --> days in 23 years\n",
        "    \"age_vocab_size\": 144,  # number of vocab for age embedding\n",
        "    \"max_position_embedding\": train_params[\"max_len_seq\"],  # maximum number of tokens\n",
        "    \"hidden_dropout_prob\": 0.1,  # dropout rate\n",
        "    \"num_hidden_layers\": 6,  # number of multi-head attention layers required\n",
        "    \"num_attention_heads\": 12,  # number of attention heads\n",
        "    \"attention_probs_dropout_prob\": 0.1,  # multi-head attention dropout rate\n",
        "    \"intermediate_size\": 512,  # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    \"hidden_act\": \"gelu\",  # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    \"initializer_range\": 0.02,  # parameter weight initializer range\n",
        "    \"optim_param\": optim_param,  # learning rate\n",
        "}\n",
        "\n",
        "# Checkopoint\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"metrics/epoch/loss_val\",\n",
        "    dirpath=file_config[\"model_path\"] + \"/checkpoints\",\n",
        "    filename=\"checkpoint-{epoch:02d}\",\n",
        ")\n",
        "\n",
        "# Define model\n",
        "neptune_logger.log_hyperparams(model_config)\n",
        "model = BertModel(BertConfig(model_config))\n",
        "task = BertMLM(model, BertConfig(model_config))\n",
        "\n",
        "# Initialize the Trainer with the callback and Neptune logger\n",
        "trainer = pl.Trainer(\n",
        "    accelerator = 'gpu',\n",
        "    logger=neptune_logger,\n",
        "    max_epochs=10,\n",
        "    log_every_n_steps=100,\n",
        "    callbacks=checkpoint_callback,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model as usual\n",
        "trainer.fit(model=task, train_dataloaders=trainload, val_dataloaders=valload)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_folder\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_vocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_bert\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mBert\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataLoaderMLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedDataset\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR/task/../dataLoader/build_vocab.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Script to build vocabulary from a dataset and save it.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691751205440
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}