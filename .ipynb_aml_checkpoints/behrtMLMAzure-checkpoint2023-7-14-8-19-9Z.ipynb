{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t   .gitignore\t\t   behrtmlmazure.ipynb.amltmp\r\n",
      "..\t\t   .ipynb_aml_checkpoints  common\r\n",
      ".DS_Store\t   .vscode\t\t   data\r\n",
      ".amlignore\t   __pycache__\t\t   dataLoader\r\n",
      ".amlignore.amltmp  behrt.yml\t\t   model\r\n",
      ".git\t\t   behrtMLMAzure.ipynb\t   task\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1692000132353
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: My-Experiment_1692000208_297c4206\n",
      "Web View: https://ml.azure.com/runs/My-Experiment_1692000208_297c4206?wsid=/subscriptions/f8c5aac3-29fc-4387-858a-1f61722fb57a/resourcegroups/forskerpl-p2ss0q-rg/workspaces/forskerpl-p2ss0q-mlw&tid=421c9c18-32c7-4250-84e0-c1626f963b1f\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2023-08-14T08:04:24.190691] Entering context manager injector.\n",
      "[2023-08-14T08:04:31.006193] context_manager_injector.py Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['task/executeBehrtMLM.py'])\n",
      "Script type = None\n",
      "[2023-08-14T08:04:31.024130] Entering Run History Context Manager.\n",
      "[2023-08-14T08:04:40.879861] Current directory: /tmp/azureml_runs/My-Experiment_1692000208_297c4206\n",
      "[2023-08-14T08:04:40.879906] Preparing to call script [task/executeBehrtMLM.py] with arguments:[]\n",
      "[2023-08-14T08:04:40.879949] After variable expansion, calling script [task/executeBehrtMLM.py] with arguments:[]\n",
      "\n",
      "\n",
      "\n",
      "[2023-08-14T08:04:40.908903] The experiment failed. Finalizing run...\n",
      "[2023-08-14T08:04:40.908920] Start FinalizingInRunHistory\n",
      "[2023-08-14T08:04:40.909985] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 4511\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "3 items cleaning up...\n",
      "Cleanup took 0.2285618782043457 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"task/executeBehrtMLM.py\", line 166, in <module>\n",
      "    main()\n",
      "  File \"task/executeBehrtMLM.py\", line 6, in main\n",
      "    import lightning.pytorch as pl\n",
      "ModuleNotFoundError: No module named 'lightning'\n",
      "\n",
      "[2023-08-14T08:04:42.136764] Finished context manager injector with Exception.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: My-Experiment_1692000208_297c4206\n",
      "Web View: https://ml.azure.com/runs/My-Experiment_1692000208_297c4206?wsid=/subscriptions/f8c5aac3-29fc-4387-858a-1f61722fb57a/resourcegroups/forskerpl-p2ss0q-rg/workspaces/forskerpl-p2ss0q-mlw&tid=421c9c18-32c7-4250-84e0-c1626f963b1f\n",
      "\n",
      "Warnings:\n",
      "Local execution of User Script failed. Details can be found in azureml-logs/60_control_log.txt log file.\n",
      "\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with ModuleNotFoundError: No module named 'lightning'\",\n        \"messageParameters\": {},\n        \"detailsUri\": \"https://aka.ms/azureml-run-troubleshooting\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with ModuleNotFoundError: No module named 'lightning'\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-run-troubleshooting\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m src \u001b[38;5;241m=\u001b[39m ScriptRunConfig(source_directory\u001b[38;5;241m=\u001b[39mscript_folder,\n\u001b[1;32m     34\u001b[0m                       script\u001b[38;5;241m=\u001b[39mscript_name,\n\u001b[1;32m     35\u001b[0m                       environment\u001b[38;5;241m=\u001b[39menv) \n\u001b[1;32m     37\u001b[0m run \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39msubmit(src)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/core/run.py:849\u001b[0m, in \u001b[0;36mRun.wait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_run_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwait_post_processing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_post_processing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_details()\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/azureml/core/run.py:1102\u001b[0m, in \u001b[0;36mRun._stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1104\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with ModuleNotFoundError: No module named 'lightning'\",\n        \"messageParameters\": {},\n        \"detailsUri\": \"https://aka.ms/azureml-run-troubleshooting\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with ModuleNotFoundError: No module named 'lightning'\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-run-troubleshooting\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import azureml._restclient.snapshots_client\n",
    "\n",
    "# Set the desired snapshot size (in bytes)\n",
    "snapshot_size = 1073741824\n",
    "\n",
    "# Update the maximum snapshot size\n",
    "azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = snapshot_size\n",
    "\n",
    "# Retrieve Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the Curated Environment\n",
    "env = Environment.get(workspace=ws, name=\"mikkel_behrt\")\n",
    "\n",
    "# Define additional conda and pip packages you want to install\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"lightning\")  # Add PyTorch Lightning\n",
    "\n",
    "env.python.conda_dependencies = conda_dep\n",
    "\n",
    "# Define the Training Script and Directory\n",
    "script_folder = ''\n",
    "script_name = 'task/executeBehrtMLM.py'  # your training script\n",
    "\n",
    "# Create an Experiment\n",
    "experiment_name = 'My-Experiment'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Configure and Submit the Training Job\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script=script_name,\n",
    "                      compute_target = 'MikkelGpu'\n",
    "                      environment=env) \n",
    "\n",
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1691833353869
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "base                     /anaconda\r\n",
      "azureml_py310_sdkv2      /anaconda/envs/azureml_py310_sdkv2\r\n",
      "azureml_py38             /anaconda/envs/azureml_py38\r\n",
      "azureml_py38_PT_TF       /anaconda/envs/azureml_py38_PT_TF\r\n",
      "behrt_cpu                /anaconda/envs/behrt_cpu\r\n",
      "jupyter_env              /anaconda/envs/jupyter_env\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda env list\n",
    "!conda activate --stack azureml_py38_PT_TF\n",
    "\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1691833283282
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sys.path.insert(0, \"../\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_folder\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_vocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_bert\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mBert\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataLoaderMLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedDataset\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkel4c14/code/Users/mikkel.sinkjaer/transformerEHR/dataLoader/build_vocab.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Script to build vocabulary from a dataset and save it.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"../\")\n",
    "from common.common import create_folder\n",
    "from dataLoader.build_vocab import build_vocab\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from dataLoader.dataLoaderMLM import MaskedDataset\n",
    "from model.behrt import BertModel, BertMLM\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import os\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import NeptuneLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "Azure = True\n",
    "\n",
    "# Initialize Neptune\n",
    "name_experiment = \"MLM_model\"\n",
    "neptune_logger = NeptuneLogger(\n",
    "    project=\"sinkjaer/BEHRT\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzOWVmOWI3Mi1jNjliLTQ3NmEtODVjMy0wZjkxZTBiMzFiMzEifQ==\",\n",
    "    log_model_checkpoints=True,\n",
    "    name=name_experiment,\n",
    ")\n",
    "\n",
    "\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get(\"vocab_size\"),\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_hidden_layers=config.get(\"num_hidden_layers\"),\n",
    "            num_attention_heads=config.get(\"num_attention_heads\"),\n",
    "            intermediate_size=config.get(\"intermediate_size\"),\n",
    "            hidden_act=config.get(\"hidden_act\"),\n",
    "            hidden_dropout_prob=config.get(\"hidden_dropout_prob\"),\n",
    "            attention_probs_dropout_prob=config.get(\"attention_probs_dropout_prob\"),\n",
    "            max_position_embeddings=config.get(\"max_position_embedding\"),\n",
    "            initializer_range=config.get(\"initializer_range\"),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get(\"seg_vocab_size\")\n",
    "        self.age_vocab_size = config.get(\"age_vocab_size\")\n",
    "        self.date_vocab_size = config.get(\"date_vocab_size\")\n",
    "        self.optim_param = config.get(\"optim_param\")\n",
    "\n",
    "\n",
    "if Azure:\n",
    "    os.environ['NEPTUNE_MODE'] = 'offline'\n",
    "    file_config = {\n",
    "        \"data_train\": \"../../EHR_data/data/pre_train_training_set.json\",  # formated data\n",
    "        \"data_val\": \"../../EHR_data/data/pre_train_validation_set.json\",  # formated data\n",
    "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
    "        \"model_name\": \"behrt\",  # model name\n",
    "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
    "        \"file_name\": \"log.txt\",  # log path\n",
    "    }\n",
    "else:\n",
    "    file_config = {\n",
    "        \"data_train\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
    "        \"data_val\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
    "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
    "        \"model_name\": \"behrt\",  # model name\n",
    "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
    "        \"file_name\": \"log.txt\",  # log path\n",
    "    }\n",
    "\n",
    "create_folder(file_config[\"model_path\"])\n",
    "\n",
    "global_params = {\"max_seq_len\": 512, \"gradient_accumulation_steps\": 1}\n",
    "\n",
    "optim_param = {\"lr\": 2e-5, \"warmup_proportion\": 0.1, \"weight_decay\": 0.01}\n",
    "\n",
    "train_params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"max_len_seq\": global_params[\"max_seq_len\"],\n",
    "}\n",
    "\n",
    "# load data\n",
    "with open(file_config[\"data_train\"]) as f:\n",
    "    data_train_json = json.load(f)\n",
    "with open(file_config[\"data_val\"]) as f:\n",
    "    data_val_json = json.load(f)\n",
    "\n",
    "# Build vocab\n",
    "vocab_path = os.path.join(file_config[\"model_path\"], file_config[\"vocab\"])\n",
    "vocab_list, word_to_idx = build_vocab(\n",
    "    data_train_json,\n",
    "    save_file=vocab_path,\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "masked_data_train = MaskedDataset(data_train_json, vocab_list, word_to_idx)\n",
    "trainload = DataLoader(\n",
    "    dataset=masked_data_train,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "masked_data_val = MaskedDataset(data_val_json, vocab_list, word_to_idx)\n",
    "valload = DataLoader(\n",
    "    dataset=masked_data_val,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "# Model config\n",
    "model_config = {\n",
    "    \"vocab_size\": len(vocab_list),  # number of disease + symbols for word embedding\n",
    "    \"hidden_size\": 288,  # word embedding and seg embedding hidden size\n",
    "    \"seg_vocab_size\": 2,  # number of vocab for seg embedding\n",
    "    \"date_vocab_size\": int(\n",
    "        365.25 * 23\n",
    "    ),  # number of vocab for dates embedding --> days in 23 years\n",
    "    \"age_vocab_size\": 144,  # number of vocab for age embedding\n",
    "    \"max_position_embedding\": train_params[\"max_len_seq\"],  # maximum number of tokens\n",
    "    \"hidden_dropout_prob\": 0.1,  # dropout rate\n",
    "    \"num_hidden_layers\": 6,  # number of multi-head attention layers required\n",
    "    \"num_attention_heads\": 12,  # number of attention heads\n",
    "    \"attention_probs_dropout_prob\": 0.1,  # multi-head attention dropout rate\n",
    "    \"intermediate_size\": 512,  # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    \"hidden_act\": \"gelu\",  # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    \"initializer_range\": 0.02,  # parameter weight initializer range\n",
    "    \"optim_param\": optim_param,  # learning rate\n",
    "}\n",
    "\n",
    "# Checkopoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"metrics/epoch/loss_val\",\n",
    "    dirpath=file_config[\"model_path\"] + \"/checkpoints\",\n",
    "    filename=\"checkpoint-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Define model\n",
    "neptune_logger.log_hyperparams(model_config)\n",
    "model = BertModel(BertConfig(model_config))\n",
    "task = BertMLM(model, BertConfig(model_config))\n",
    "\n",
    "# Initialize the Trainer with the callback and Neptune logger\n",
    "trainer = pl.Trainer(\n",
    "    accelerator = 'gpu',\n",
    "    logger=neptune_logger,\n",
    "    max_epochs=10,\n",
    "    log_every_n_steps=100,\n",
    "    callbacks=checkpoint_callback,\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model as usual\n",
    "trainer.fit(model=task, train_dataloaders=trainload, val_dataloaders=valload)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
