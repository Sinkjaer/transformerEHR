{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment, Workspace\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "\n",
        "# Retrieve Workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Retrieve the environment (assuming you've already connected to the workspace)\n",
        "env = Environment.get(workspace=ws, name=\"AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\")\n",
        "\n",
        "!conda env create -f behrt.yml\n",
        "\n",
        "# Create a Conda environment from the file\n",
        "conda_env_name = 'behrt'\n",
        "\n",
        "# Activate the Conda environment (you may need to do this manually in a new terminal)\n",
        "!source activate {conda_env_name}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- "
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691764433293
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "# sys.path.insert(0, \"../\")\n",
        "from common.common import create_folder\n",
        "from dataLoader.build_vocab import build_vocab\n",
        "import pytorch_pretrained_bert as Bert\n",
        "from dataLoader.dataLoaderMLM import MaskedDataset\n",
        "from model.behrt import BertModel, BertMLM\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import os\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.loggers import NeptuneLogger\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.tuner import Tuner\n",
        "\n",
        "Azure = True\n",
        "\n",
        "# Initialize Neptune\n",
        "name_experiment = \"MLM_model\"\n",
        "neptune_logger = NeptuneLogger(\n",
        "    project=\"sinkjaer/BEHRT\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzOWVmOWI3Mi1jNjliLTQ3NmEtODVjMy0wZjkxZTBiMzFiMzEifQ==\",\n",
        "    log_model_checkpoints=True,\n",
        "    name=name_experiment,\n",
        ")\n",
        "\n",
        "\n",
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get(\"vocab_size\"),\n",
        "            hidden_size=config[\"hidden_size\"],\n",
        "            num_hidden_layers=config.get(\"num_hidden_layers\"),\n",
        "            num_attention_heads=config.get(\"num_attention_heads\"),\n",
        "            intermediate_size=config.get(\"intermediate_size\"),\n",
        "            hidden_act=config.get(\"hidden_act\"),\n",
        "            hidden_dropout_prob=config.get(\"hidden_dropout_prob\"),\n",
        "            attention_probs_dropout_prob=config.get(\"attention_probs_dropout_prob\"),\n",
        "            max_position_embeddings=config.get(\"max_position_embedding\"),\n",
        "            initializer_range=config.get(\"initializer_range\"),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get(\"seg_vocab_size\")\n",
        "        self.age_vocab_size = config.get(\"age_vocab_size\")\n",
        "        self.date_vocab_size = config.get(\"date_vocab_size\")\n",
        "        self.optim_param = config.get(\"optim_param\")\n",
        "\n",
        "\n",
        "if Azure:\n",
        "    os.environ['NEPTUNE_MODE'] = 'offline'\n",
        "    file_config = {\n",
        "        \"data_train\": \"../../EHR_data/data/pre_train_training_set.json\",  # formated data\n",
        "        \"data_val\": \"../../EHR_data/data/pre_train_validation_set.json\",  # formated data\n",
        "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
        "        \"model_name\": \"behrt\",  # model name\n",
        "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "    }\n",
        "else:\n",
        "    file_config = {\n",
        "        \"data_train\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
        "        \"data_val\": \"/Users/mikkelsinkjaer/data/data.json\",\n",
        "        \"model_path\": \"MLM/\" + name_experiment,  # where to save model\n",
        "        \"model_name\": \"behrt\",  # model name\n",
        "        \"vocab\": \"vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "    }\n",
        "\n",
        "create_folder(file_config[\"model_path\"])\n",
        "\n",
        "global_params = {\"max_seq_len\": 512, \"gradient_accumulation_steps\": 1}\n",
        "\n",
        "optim_param = {\"lr\": 2e-5, \"warmup_proportion\": 0.1, \"weight_decay\": 0.01}\n",
        "\n",
        "train_params = {\n",
        "    \"batch_size\": 128,\n",
        "    \"max_len_seq\": global_params[\"max_seq_len\"],\n",
        "}\n",
        "\n",
        "# load data\n",
        "with open(file_config[\"data_train\"]) as f:\n",
        "    data_train_json = json.load(f)\n",
        "with open(file_config[\"data_val\"]) as f:\n",
        "    data_val_json = json.load(f)\n",
        "\n",
        "# Build vocab\n",
        "vocab_path = os.path.join(file_config[\"model_path\"], file_config[\"vocab\"])\n",
        "vocab_list, word_to_idx = build_vocab(\n",
        "    data_train_json,\n",
        "    save_file=vocab_path,\n",
        ")\n",
        "\n",
        "# Data loader\n",
        "masked_data_train = MaskedDataset(data_train_json, vocab_list, word_to_idx)\n",
        "trainload = DataLoader(\n",
        "    dataset=masked_data_train,\n",
        "    batch_size=train_params[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=6,\n",
        ")\n",
        "masked_data_val = MaskedDataset(data_val_json, vocab_list, word_to_idx)\n",
        "valload = DataLoader(\n",
        "    dataset=masked_data_val,\n",
        "    batch_size=train_params[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=6,\n",
        ")\n",
        "\n",
        "# Model config\n",
        "model_config = {\n",
        "    \"vocab_size\": len(vocab_list),  # number of disease + symbols for word embedding\n",
        "    \"hidden_size\": 288,  # word embedding and seg embedding hidden size\n",
        "    \"seg_vocab_size\": 2,  # number of vocab for seg embedding\n",
        "    \"date_vocab_size\": int(\n",
        "        365.25 * 23\n",
        "    ),  # number of vocab for dates embedding --> days in 23 years\n",
        "    \"age_vocab_size\": 144,  # number of vocab for age embedding\n",
        "    \"max_position_embedding\": train_params[\"max_len_seq\"],  # maximum number of tokens\n",
        "    \"hidden_dropout_prob\": 0.1,  # dropout rate\n",
        "    \"num_hidden_layers\": 6,  # number of multi-head attention layers required\n",
        "    \"num_attention_heads\": 12,  # number of attention heads\n",
        "    \"attention_probs_dropout_prob\": 0.1,  # multi-head attention dropout rate\n",
        "    \"intermediate_size\": 512,  # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    \"hidden_act\": \"gelu\",  # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    \"initializer_range\": 0.02,  # parameter weight initializer range\n",
        "    \"optim_param\": optim_param,  # learning rate\n",
        "}\n",
        "\n",
        "# Checkopoint\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"metrics/epoch/loss_val\",\n",
        "    dirpath=file_config[\"model_path\"] + \"/checkpoints\",\n",
        "    filename=\"checkpoint-{epoch:02d}\",\n",
        ")\n",
        "\n",
        "# Define model\n",
        "neptune_logger.log_hyperparams(model_config)\n",
        "model = BertModel(BertConfig(model_config))\n",
        "task = BertMLM(model, BertConfig(model_config))\n",
        "\n",
        "# Initialize the Trainer with the callback and Neptune logger\n",
        "trainer = pl.Trainer(\n",
        "    accelerator = 'gpu',\n",
        "    logger=neptune_logger,\n",
        "    max_epochs=10,\n",
        "    log_every_n_steps=100,\n",
        "    callbacks=checkpoint_callback,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model as usual\n",
        "trainer.fit(model=task, train_dataloaders=trainload, val_dataloaders=valload)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_folder\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_vocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_bert\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mBert\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataLoader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataLoaderMLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedDataset\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/mikkelgpu/code/Users/mikkel.sinkjaer/transformerEHR/dataLoader/build_vocab.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Script to build vocabulary from a dataset and save it.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691762930308
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}