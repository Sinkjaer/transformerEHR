{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"../\")\n",
        "\n",
        "Azure = True\n",
        "\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from dataLoader.dataLoaderMLM import CoercionRiskDataset, process_data_CoercionRisk\n",
        "from dataLoader.build_vocab import load_vocab, build_vocab"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1690537138324
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Azure:\n",
        "    file_config = {\n",
        "        \"vocab\": \"../dataloader/vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"data_train\": \"../../EHR_data/data/fine_tune_train_set.json\",  # formated data\n",
        "        \"data_val\": \"../../EHR_data/CoercionData_val.json\",  # formated data\n",
        "        \"model_path\": \"model/model1/\",  # where to save model\n",
        "        \"model_name\": \"test\",  # model name\n",
        "        \"file_name\": \"log\",  # log path\n",
        "        \"use_cuda\": True,\n",
        "        \"device$\": \"cuda:0\",\n",
        "    }\n",
        "else:\n",
        "    file_config = {\n",
        "        \"vocab\": \"/Users/mikkelsinkjaer/Library/Mobile Documents/com~apple~CloudDocs/transEHR/Code/transformerEHR/data/vocab.txt\",  # vocabulary idx2token, token2idx\n",
        "        \"data_train\": \"/Users/mikkelsinkjaer/Library/Mobile Documents/com~apple~CloudDocs/transEHR/Code/transformerEHR/data/syntheticData.json\",\n",
        "        \"data_val\": \"/Users/mikkelsinkjaer/Library/Mobile Documents/com~apple~CloudDocs/transEHR/Code/transformerEHR/data/syntheticData_val.json\",\n",
        "        \"model_path\": \"model/model1/\",  # where to save model\n",
        "        \"model_name\": \"test\",  # model name\n",
        "        \"file_name\": \"log.txt\",  # log path\n",
        "        \"use_cuda\": False,\n",
        "        \"device\": \"cpu\",\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1690537138450
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_config[\"data_train\"]) as f:\n",
        "    data_json = json.load(f)\n",
        "print(next(iter(data_json.items())))\n",
        "vocab_list, word_to_idx = build_vocab(data_json, Azure=Azure)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../../EHR_data/data/fine_tune_training_set.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     data_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_json\u001b[38;5;241m.\u001b[39mitems())))\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../EHR_data/data/fine_tune_training_set.json'"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1690537138631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader\r\n",
        "data = process_data_CoercionRisk(\r\n",
        "    data_json, vocab_list, word_to_idx, mask_prob=0.20, Azure=Azure\r\n",
        ")\r\n",
        "sample_data = CoercionRiskDataset(data)\r\n",
        "sample = next(iter(sample_data))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690537138716
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\r\n",
        "\r\n",
        "# sample = next(iter(data.items()))\r\n",
        "\r\n",
        "# Replace 'N' with the desired item number you want to access (in this case, 10)\r\n",
        "item_number = 10100\r\n",
        "\r\n",
        "# Using itertools.islice to get the Nth item (key-value pair) from the dictionary\r\n",
        "# sample = next(itertools.islice(data.items(), item_number))\r\n",
        "\r\n",
        "count  = 0\r\n",
        "for sample in data.items():\r\n",
        "    index = sample[1]['sample_index']\r\n",
        "    if sample[1]['classification_labels'] == 1:\r\n",
        "        print('date',sample[1]['dates'][index-5:index+5])\r\n",
        "        print('age',sample[1]['age'][index-5:index+5])\r\n",
        "        print('codes',sample[1]['codes'][index-5:index+5])\r\n",
        "        print('position',sample[1]['position'][index-5:index+5])\r\n",
        "        print('segment',sample[1]['segment'][index-5:index+5])\r\n",
        "        print('classification_labels',sample[1]['classification_labels'])\r\n",
        "        print('original_code_sequence',sample[1]['original_code_sequence'][index-5:index+5])\r\n",
        "        print('sample_index',sample[1]['sample_index'])\r\n",
        "        print('input_sequence',sample[1]['input_sequence'][index-5:index+5])\r\n",
        "        print('attention_mask',sample[1]['attention_mask'][index-5:index+5])\r\n",
        "        break\r\n",
        "    count +=1\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690537138730
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}